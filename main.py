import os
import time
import pandas as pd
from sklearn.model_selection import train_test_split
import re
import torch
import torch.nn as nn
import torch.optim as optim
from models import CharLSTMGenerator
# from pytorch_lightning import Trainer
# from pytorch_lightning.callbacks import ModelCheckpoint
from torch.utils.data import DataLoader
from dataset import load_and_clean_data, save_split, PeptideDataset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

timestamp = time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime())
start_time = time.time()

# === é…ç½®éƒ¨åˆ† ===
INPUT_FILE = "data/peptides.csv"  # åŸå§‹æ•°æ®è·¯å¾„
OUTPUT_DIR = "data/splits"        # è®­ç»ƒéªŒè¯é›†è¾“å‡ºè·¯å¾„

if __name__ == "__main__":
    df = load_and_clean_data(INPUT_FILE)
    # print(f"å…±è½½å…¥ {len(df)} æ¡åˆæ³•å¤šè‚½åºåˆ—ï¼ˆé•¿åº¦ â‰¥ æŒ‡å®šé•¿åº¦ï¼‰ã€‚")
    save_split(df, OUTPUT_DIR)

def load_sequences_from_file(path):
    with open(path, 'r') as f:
        return [line.strip() for line in f if line.strip()]


# è¶…å‚æ•°
seq_length = 20
hidden_dim = 128
num_layers = 1
batch_size = 16
epochs = 1
learning_rate = 0.003
# è®°å½•æµ‹è¯•çš„æ¬¡æ•°
total_test_step = 0
# è®­ç»ƒé›†ï¼šåŠ è½½è®­ç»ƒé›†åºåˆ—
train_path = os.path.join(OUTPUT_DIR, "train.txt")
sequences = load_sequences_from_file(train_path)

# æ„å»ºå­—ç¬¦è¡¨
# chars = sorted(list(set("".join(sequences))))
# char2idx = {ch: idx for idx, ch in enumerate(chars)}
amino_acids = list("ACDEFGHIKLMNPQRSTVWY")      # æ ‡å‡†20ç§æ°¨åŸºé…¸å•å­—æ¯ä»£ç ï¼Œé¡ºåºå¯è‡ªç”±å®šä¹‰
char2idx = {ch: idx for idx, ch in enumerate(amino_acids)}
idx2char = {idx: ch for ch, idx in char2idx.items()}
vocab_size = len(chars)

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
train_set = PeptideDataset(sequences, seq_length=seq_length, char2idx=char2idx)
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)

# éªŒè¯é›†ï¼šåŠ è½½è®­ç»ƒé›†åºåˆ—
val_path = os.path.join(OUTPUT_DIR, "val.txt")
sequences = load_sequences_from_file(val_path)
val_set = PeptideDataset(sequences, seq_length=seq_length, char2idx=char2idx)
val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)


# æ¨¡å‹
model = CharLSTMGenerator(vocab_size, hidden_dim, vocab_size, num_layers)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

checkpoint_path = "checkpoint.pt"
start_epoch = 0

if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    print(f"ä»checkpoint {checkpoint_path} ç»§ç»­è®­ç»ƒï¼Œèµ·å§‹epoch: {start_epoch}")
    epochs = epochs + start_epoch
else:
    print("æœªæ‰¾åˆ°checkpointï¼Œè®­ç»ƒå°†ä»å¤´å¼€å§‹")

def validate(model, dataloader, criterion):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            output, _ = model(x)
            output = output.view(-1, output.size(-1))
            y = y.view(-1)
            loss = criterion(output, y)
            total_loss += loss.item()
    avg_loss = total_loss / len(dataloader)
    print(f"éªŒè¯é›†å¹³å‡Loss: {avg_loss:.4f}")
    model.train()
    return avg_loss

def train(model, train_loader, val_loader, optimizer, criterion, start_epoch=0, num_epochs=10):
    # è®°å½•è®­ç»ƒçš„æ¬¡æ•°
    total_train_step = 0

    # === åˆå§‹åŒ–è®°å½•ç»“æ„ ===
    train_losses = []
    val_losses = []
    best_val_loss = checkpoint.get('val_loss_best', float('inf'))
    best_model_state = None                         # ç”¨äºå­˜å‚¨æœ€ä½³æ¨¡å‹å‚æ•°
    best_epoch = 0

    for epoch in range(start_epoch, num_epochs):
        print("-------ç¬¬{}è½®è®­ç»ƒå¼€å§‹-------".format(epoch + 1))
        total_loss = 0

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}", leave=False)

        for x, y in progress_bar:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            output, _ = model(x)
            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_train_step += 1

            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            progress_bar.set_postfix(loss=loss.item())

            # if total_train_step % 250 == 0:
            #     end_time = time.time()
            #     print(end_time - start_time)
            #     print("è®­ç»ƒæ¬¡æ•°ï¼š{}, Lossï¼š{}".format(total_train_step, loss.item()))

            # TensorBoardè®°å½•
            writer.add_scalar('train_loss', loss.item(), total_train_step)

        avg_train_loss = total_loss / len(train_loader)

        # éªŒè¯
        avg_val_loss = validate(model, val_loader, criterion)

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹å‚æ•°
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_state = model.state_dict()
            best_epoch = epoch + 1

        # æ‰“å°æˆ–ä¿å­˜
        print(f"Epoch {epoch + 1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        torch.save(best_model_state, "best_model.pt")
        print(f"ğŸ¯ æœ€ä¼˜æ¨¡å‹æ¥è‡ª Epoch {best_epoch}, Val Loss: {best_val_loss:.4f}")

        # ä¿å­˜ checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': total_loss,
            'val_loss_best': best_val_loss
        }, "checkpoint.pt")


# æ‹¼æ¥è·¯å¾„
log_dir = os.path.join("logs", f"run-{timestamp}")

writer = SummaryWriter(log_dir=log_dir)

# è°ƒç”¨è®­ç»ƒå‡½æ•°
train(model, train_loader, val_loader, optimizer, criterion, start_epoch=start_epoch, num_epochs=epochs)

writer.close()
# tensorboard --logdir=log_dir
end_time = time.time()
print(f"è„šæœ¬è¿è¡Œäº† {end_time - start_time:.0f} ç§’é’Ÿ")


