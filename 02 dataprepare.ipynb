{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# === 配置部分 ===\n",
    "INPUT_FILE = \"data/peptides.csv\"  # 原始数据路径\n",
    "OUTPUT_DIR = \"data/splits\"        # 训练验证集输出路径\n",
    "VALID_AMINO_ACIDS = set(\"ACDEFGHIKLMNPQRSTVWY\")  # 标准20种氨基酸\n",
    "MIN_LENGTH = 10  # 你可以调节这个阈值\n",
    "\n",
    "def is_valid_Sequence(seq):\n",
    "    \"\"\"只保留标准氨基酸组成的序列\"\"\"\n",
    "    if not isinstance(seq, str):\n",
    "        return False\n",
    "    return all(residue in VALID_AMINO_ACIDS for residue in seq)\n",
    "\n",
    "def load_and_clean_data(file_path):\n",
    "    \"\"\"加载CSV文件并清洗非法序列\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"读取列名: {df.columns.tolist()}\")\n",
    "\n",
    "    df['Sequence'] = df['Sequence'].str.upper()\n",
    "    df = df[df['Sequence'].apply(is_valid_Sequence)]\n",
    "    df = df[df['Sequence'].str.len() >= MIN_LENGTH]  # 过滤长度\n",
    "    return df\n",
    "\n",
    "def save_split(df, output_dir, train_ratio=0.8):\n",
    "    \"\"\"划分训练/验证集并保存为txt文件\"\"\"\n",
    "    # train_df, val_df = train_test_split(df, train_size=train_ratio, random_state=42, stratify=df['label']) 现在没有label\n",
    "    train_df, val_df = train_test_split(df, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    train_path = os.path.join(output_dir, \"train.txt\")\n",
    "    val_path = os.path.join(output_dir, \"val.txt\")\n",
    "\n",
    "    train_df.to_csv(train_path, sep='\\t', index=False, header=True)\n",
    "    val_df.to_csv(val_path, sep='\\t', index=False, header=True)\n",
    "\n",
    "    print(f\"✅ 数据已成功拆分并保存：\\n - {train_path}\\n - {val_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_and_clean_data(INPUT_FILE)\n",
    "    print(f\"共载入 {len(df)} 条合法多肽序列（长度 ≥ {MIN_LENGTH}）。\")\n",
    "    save_split(df, OUTPUT_DIR)\n",
    "\n"
   ],
   "id": "c1fd98fb1da034fa",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n"
   ],
   "id": "a10b2f924aa780ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:10:26.483343Z",
     "start_time": "2025-07-22T14:10:26.469471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 示例多肽数据（你应替换为真实序列）\n",
    "sequences = ['ACDEFGHIKLMNPQRSTVWY', 'MKTIIALSYIFCLVFAD', 'GAVLIMFWP']\n",
    "\n",
    "# 获取全部字符（氨基酸）集合\n",
    "all_chars = sorted(set(''.join(sequences)))\n",
    "vocab_size = len(all_chars)\n",
    "\n",
    "# 建立字符与索引的映射\n",
    "char2idx = {ch: i for i, ch in enumerate(all_chars)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n"
   ],
   "id": "699cc29c2d1a6a72",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:11:18.835097Z",
     "start_time": "2025-07-22T14:11:18.818895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(all_chars)\n",
    "print(char2idx)"
   ],
   "id": "4e700e0fe04a0040",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
      "{'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:11:36.417721Z",
     "start_time": "2025-07-22T14:11:36.407872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PeptideDataset(Dataset):\n",
    "    def __init__(self, sequences, seq_length=20):\n",
    "        self.data = []\n",
    "        for seq in sequences:\n",
    "            for i in range(len(seq) - seq_length):\n",
    "                input_seq = seq[i:i+seq_length]\n",
    "                target_seq = seq[i+1:i+seq_length+1]\n",
    "                self.data.append((input_seq, target_seq))\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.data[idx]\n",
    "        x = torch.zeros(self.seq_length, vocab_size)\n",
    "        y = torch.zeros(self.seq_length, dtype=torch.long)\n",
    "        for i, ch in enumerate(input_seq):\n",
    "            x[i][char2idx[ch]] = 1.0\n",
    "        for i, ch in enumerate(target_seq):\n",
    "            y[i] = char2idx[ch]\n",
    "        return x, y\n"
   ],
   "id": "c9f04401332b2ec9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:14:15.161740Z",
     "start_time": "2025-07-22T14:14:15.147451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharLSTMGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(CharLSTMGenerator, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n"
   ],
   "id": "2f5a2295da750727",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:15:38.204792Z",
     "start_time": "2025-07-22T14:15:38.160984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 超参数\n",
    "seq_length = 20\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "learning_rate = 0.003\n",
    "\n",
    "# 数据加载\n",
    "dataset = PeptideDataset(sequences, seq_length=seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 模型\n",
    "model = CharLSTMGenerator(vocab_size, hidden_dim, vocab_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ],
   "id": "3bea537636e63779",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# 数据加载\u001B[39;00m\n\u001B[0;32m     10\u001B[0m dataset \u001B[38;5;241m=\u001B[39m PeptideDataset(sequences, seq_length\u001B[38;5;241m=\u001B[39mseq_length)\n\u001B[1;32m---> 11\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m \u001B[43mDataLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# 模型\u001B[39;00m\n\u001B[0;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m CharLSTMGenerator(vocab_size, hidden_dim, vocab_size, num_layers)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\bioai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:383\u001B[0m, in \u001B[0;36mDataLoader.__init__\u001B[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001B[0m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# map-style\u001B[39;00m\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m shuffle:\n\u001B[1;32m--> 383\u001B[0m         sampler \u001B[38;5;241m=\u001B[39m \u001B[43mRandomSampler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    384\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    385\u001B[0m         sampler \u001B[38;5;241m=\u001B[39m SequentialSampler(dataset)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\bioai\\lib\\site-packages\\torch\\utils\\data\\sampler.py:165\u001B[0m, in \u001B[0;36mRandomSampler.__init__\u001B[1;34m(self, data_source, replacement, num_samples, generator)\u001B[0m\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    161\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreplacement should be a boolean value, but got replacement=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplacement\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    162\u001B[0m     )\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 165\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    166\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    167\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e730ac777bd136a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
